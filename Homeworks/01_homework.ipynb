{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "102c7985",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/DavideScassola/PML2024/blob/main/./Homeworks/01_homework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabilistic Machine Learning -- Spring 2024, UniTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call $S$ the Bernoulli random variable describing the presence of a certain substance (S=1: present, S=0: not present) and $T$ the Bernoulli random variable describing the result of the test for detecting that substance (T=1: positive, T=0: negative). \n",
    "\n",
    "Given:\n",
    "- $P(T=1 | S=1) = 1 - 10^{-4}$\n",
    "- $P(T=1 | S=0) = 10^{-3}$\n",
    "- $P(S = 1) = 10^{-4}$\n",
    "\n",
    "1. Compute the normalized mutual information, that is defined as follows:\n",
    "$$\\text{NMI}[S,T] = \\frac{2 \\cdot \\text{I}[S,T]}{\\text{H}[A] + \\text{H}[B]}$$\n",
    "\n",
    "where $\\text{I}$ indicates the mutual information, and $\\text{H}$ indicates the entropy\n",
    "\n",
    "2. Let's suppose one can repeat the same test $n$ times, and the result of each test will be independent (conditionally given S). What is the minimum number of tests $n$ such that the probability of having a false positive after getting $n$ positive tests is less than $10^{-6}$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's night and you are looking into the sky waiting to see a falling star. After the first hour you still haven't seen anything, so you check online and find two sources $s_1$ and $s_2$. According to $s_1$ the waiting time is distributed as $p_1(t) = 2e^{-2t}$, according to $s_2$ it's $p_2(t) = 0.3e^{-0.3t}$. \n",
    "\n",
    "Assuming (only) one of the two is correct and that at first you don't trust one more than the other ( $P(s_1 \\text{ is correct}) = P(s_2 \\text{ is correct}) = 0.5$ ):\n",
    "- Which one of the two sources do you believe more after the first hour? Can you quantify it?\n",
    "- What is the probability of seeing the first falling star in the next 1 hour?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You enrolled to a small tennis tournament organized by your university, that has only other three participants: let's call them $A$, $B$ and $C$.\n",
    "Your first match will be against $A$, and it's scheduled after the match between $A$ and $B$ and the match between $B$ and $C$.\n",
    "\n",
    "Assuming the result of a match $M \\in \\{0,1\\}$ between two players $X$ and $Y$ ($M=1$ means $X$ won, $M=0$ means $Y$ won) is described by the following model:\n",
    "\n",
    "$$M \\sim Bern(p)$$\n",
    "\n",
    "where $p = f(2(S_x - S_y))$ with $f(k) = \\frac{1}{1 + e^{-k}}$ and\n",
    "\n",
    "$$S_i \\sim \\mathcal{N}(0,1)$$\n",
    "\n",
    "is the \"latent\" skill of player $i$ (always the same for every match that player $i$ plays)\n",
    "\n",
    "1. Show a bayesian network describing the relationship between all the involved random variables.\n",
    "2. Make a model in pyro describing the stochastic process.\n",
    "3. Estimate by simulation the probability of (you) winninng against $A$, given that $A$ won against $B$ and $B$ won against $C$. Use exactly 30000 samples and call `set_seed()` before sampling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "    \n",
    "def set_seed():\n",
    "    seed = 0\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a list of $n$ random variables $X_1, X_2, \\ldots, X_n$ such that:\n",
    "$$\\forall{i}, \\ p(x_{i+2} | x_{i+1}, x_{i}) = p(x_{i+2} | x_{i+1})$$\n",
    "\n",
    "prove:\n",
    "$$\\forall{i}, \\ p(x_{i-2} | x_{i-1}, x_{i}) = p(x_{i-2} | x_{i-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A chocolate easter egg contains 1 of $N$ possible different surprises. Assuming all surprises are equally probable, how many eggs do you expect you have to buy if you want to collect all $N$ possible surprises?\n",
    "\n",
    "Finally, compute it for $N=100$ (using python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the notebook “Exact inference with Belief Propagation”, we implemented the forward pass of the sum-product algorithm in order to compute the marginal distribution of a variable.  \n",
    "1. Add to the class “Messages” a method “forward” which computes the forward pass without calculating the marginal distribution of a given variable\n",
    "2. Add to the class “Messages” a method “backward” which computes the backward pass\n",
    "3. Add to the class “Messages” a method “belief_propagation” which executes first the forward pass and then the backward pass and uses the messages to compute  all the marginals. Return a dictionary containing the variable names and the corresponding marginals.\n",
    "4. Use this method to compute the marginals of the variables of the factor graph described on page 43 of the notes of the course. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
