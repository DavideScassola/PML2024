{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: **Probability** basics\n",
    "\n",
    "Probabilistic Machine Learning -- Spring 2024, UniTS\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/DavideScassola/PML2024/blob/main/Notebooks/01_probability_basics.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Random variables\n",
    "\n",
    "**Operational definitions**\n",
    "\n",
    "*Random processes* involve uncertainty due to chance, contrasting with *deterministic processes* that yield predictable outcomes.\n",
    "\n",
    "*Random variables* serve as abstract representations of outcomes in randomized experiments: they are functions mapping experimental results to real numbers.\n",
    "\n",
    "Random variables are not directly observable: what we can observe are the *realizations*, i.e. the result of applying the r.v., so a function, to an observed outcome of a random experiment.\n",
    "\n",
    "The domain of a random variable is the *sample space*.\n",
    "\n",
    "**Probability mass function** $p$\n",
    "\n",
    "- Finite or countable $\\mathcal{X}$;\n",
    "- $p_X(x):=P(X=x)$ for $x\\in\\mathcal{X}$.\n",
    "- Notation: instead of $p_X(x)$ we mostly use $p(x)$\n",
    "\n",
    "\n",
    "**Probability density function** $f$:\n",
    "\n",
    "- Infinite $\\mathcal{X}$;\n",
    "- Measurable function $f_X:\\mathcal{X}\\rightarrow[0,+\\infty)$;\n",
    "- $P(a \\leq X \\leq b) = \\int_a^b f_X(x)dx$.\n",
    "- Notation: instead of $f_X(x)$ we mostly use $p(x)$\n",
    "\n",
    "It follows that $\\int_\\mathbb{R} f_X(x)dx=1$.\n",
    "\n",
    "\n",
    "### Notable probability distributions\n",
    "\n",
    "\n",
    "| discrete distribution | *pmf* | mean | variance |\n",
    "| :--------------------:|:-----:|:----:|:--------:|\n",
    "| Binomial $$\\text{Bin}(n,p)$$ | $$ {n \\choose x} p^x (1-p)^{n-x}$$ | $$np$$ | $$np(1-p)$$ |\n",
    "| Bernoulli $$\\text{Bern}(p)$$| $$\\begin{cases}1-p &k=1\\\\ 0&k=0\\end{cases}$$ | $$p$$ |$$p(1-p)$$ |\n",
    "| Discrete Uniform $$\\mathcal{U}(a,b)$$ | $$\\frac{1}{b-a+1}$$ | $$\\frac{b+a}{2}$$ |$$\\frac{(b-a+1)^2-1}{12}$$ |\n",
    "| Geometric $$\\text{Geom}(p)$$ | $(1-p)^{k-1}p$ |$$\\frac{1}{p}$$|$$\\frac{1-p}{p^2}$$ |\n",
    "| Poisson $$\\text{Pois}(\\lambda)$$ |$$\\frac{\\lambda^k e^{-\\lambda}}{k!}$$|$$\\lambda$$ | $$\\lambda$$ |\n",
    "\n",
    "where:\n",
    "- $n\\in\\{0,1,2,...\\}$\n",
    "- $p \\in [0,1]$ or $p \\in (0,1)$\n",
    "- $b\\geq a$\n",
    "- $k\\in\\{1,2,3,...\\}$\n",
    "- $\\lambda \\in \\mathbb{R}^+$\n",
    "\n",
    "| continuous distribution | *pdf* | mean | variance |\n",
    "| :----------------------:|:-----:|:----:|:--------:|\n",
    "| Continuous Uniform $$\\mathcal{U}(a,b)$$|$$\\begin{cases}\\frac{1}{b-a} & x \\in [a,b]\\\\0 & \\text{otherwise}\\end{cases}$$|$$\\frac{a+b}{2}$$|$$\\frac{(b-a)^2}{12}$$ |\n",
    "| Exponential $$\\text{Exp}(\\lambda)$$|$$\\lambda e^{-\\lambda x}$$|$$1/\\lambda$$|$$1/\\lambda^2$$ |\n",
    "| Gaussian $$\\mathcal{N}(\\mu,\\sigma^2)$$|$$\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\big(\\frac{x-\\mu}{\\sigma}\\big)^2}$$|$$\\mu$$|$$\\sigma^2$$|\n",
    "|Beta $$\\text{Beta}(\\alpha,\\beta)$$|$$\\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{B(\\alpha,\\beta)}$$|$$\\frac{\\alpha}{\\alpha+\\beta}$$|$$\\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}$$| \n",
    "|Gamma $$\\text{Gamma}(\\alpha, \\beta)$$|$$\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}x^{\\alpha-1}e^{-\\beta x}$$|$$\\frac{\\alpha}{\\beta}$$|$$\\frac{\\alpha}{\\beta^2}$$|\n",
    "|Dirichlet $$Dir(\\alpha)$$|$$\\frac{1}{B(\\alpha)}\\prod_{i=1}^{K}x_i^{\\alpha_i-1}$$|$$\\tilde{\\alpha}_i$$|$$\\frac{\\tilde{\\alpha}_i(1-\\tilde{\\alpha}_i)}{\\alpha_0+1}$$ |\n",
    "|Student's t $$St(\\nu)$$| $$\\frac{\\Gamma(\\frac{\\nu+1}{2})}{\\sqrt{\\nu\\pi}\\Gamma(\\frac{\\nu}{2})}{\\Big(1+\\frac{x^2}{\\nu}\\Big)^{-\\frac{\\nu+1}{2}}}$$ |$$0$$|$$\\begin{cases}\\frac{\\nu}{\\nu-2}&\\nu>2\\\\\\infty&1<\\nu\\leq2\\end{cases}$$ |\n",
    "\n",
    "where:\n",
    "- $b \\geq a$\n",
    "- $\\lambda \\in \\mathbb{R}^+$\n",
    "- $\\mu,\\sigma,\\alpha,\\beta\\in\\mathbb{R}$\n",
    "- $\\alpha,\\beta>0$ for the Gamma distribution\n",
    "- $k,\\theta > 0$\n",
    "- $K\\in\\mathbb{Z}_{\\geq2}$\n",
    "- $\\tilde{\\alpha}_i=\\frac{\\alpha_i}{\\sum_{h=1}^K\\alpha_h}$, $\\alpha_0=\\sum_{i=1}^K \\alpha_i$\n",
    "- $\\nu>1$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Expected value\n",
    "\n",
    "**Definition**\n",
    "\n",
    "Let $X$ be a random variable.\n",
    "\n",
    "|values|expectation $E[X]$|\n",
    "|:----:|:----------------:|\n",
    "|finite| $$\\sum_{i=1}^k x_i p(x_i)$$|\n",
    "|countable|$$\\sum_{i=1}^\\infty x_i p(x_i)$$|\n",
    "|continuous|$$\\int_{\\mathbb{R}}x p(x)dx$$|\n",
    "\n",
    "where $p$ is the probability mass function of $X$ in the discrete case and the probability density function of $X$ in the continuous case. \n",
    "\n",
    "**Example: discrete case**\n",
    "\n",
    "Let $Y$ be a discrete random variable with values in $\\{0,1\\}$ and let $P(Y=1)=p$. Suppose we want to compute the expectation $\\mathbb{E}[|Y-p|]$.\n",
    "\n",
    "From the definition of expectation we know that, in the discrete case, we just need to multiply each possible value that the random variable can assume by its probability of occurring:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[|Y-p|] = p(1-p) + (1-p) p = 2p(1-p)\n",
    "$$\n",
    "\n",
    "**Example: continuous case**\n",
    "\n",
    "Let the pdf of $X$ be \n",
    "\n",
    "$$f(x)=\\begin{cases}cx^2(1-x) & 0\\leq x \\leq 1\\\\ 0 & \\text{otherwise}\\end{cases}$$\n",
    "\n",
    "We want to determine $c\\in\\mathbb{R}$ such that $f(x)$ is a valid *pdf*:\n",
    "\n",
    "$$1 =\\int_0^1 cx^2(1-x)dx = c \\int_0^1 (x^2-x^3)dx = c\\Big[\\frac{x^3}{3}-\\frac{x^4}{4}\\Big]_0^1 = \\frac{c}{12} $$\n",
    "\n",
    "$$\\Longrightarrow c=12$$\n",
    "\n",
    "Now we compute the expected value of $X$:\n",
    "\n",
    "$$E[X]=12\\int_0^1x^3(1-x)dx=12\\Big[\\frac{x^4}{4}-\\frac{x^5}{5}\\Big]_0^1=\\frac{3}{5}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marginal and conditional distributions\n",
    "\n",
    "**Definitions**\n",
    "\n",
    "Multiple random variables $X_1,\\ldots, X_N$ on the same probability space define a **multivariate random variable**, whose **joint probability mass function** is -- in the discrete case:\n",
    "\n",
    "$$p_{X_1,\\ldots, X_N}(x_1,\\ldots, x_N)=P(X_1=x_1,\\ldots,X_N=x_N)$$\n",
    "\n",
    "\n",
    "While the **joint probability density function** is -- in the continuous case:\n",
    "\n",
    "$$P(X_1\\in[a_1,b_1],\\ldots, X_N\\in[a_N,b_N])=\\int_{a_1}^{b_1}\\ldots\\int_{a_N}^{b_N}f_{X_1,\\ldots, X_N}(x_1,\\ldots,x_N)dx_1\\ldots dx_N$$\n",
    "\n",
    "\n",
    "![](./img/multivariate_normal_sample.png)\n",
    "<br><sub><sup>From <a href=\"https://en.wikipedia.org/wiki/Joint_probability_distribution\">Wikipedia: Joint probability distribution</a></sup></sub>\n",
    "\n",
    "\n",
    "In the bivariate case, for example, we can derive marginal and conditional distributions from the joint distribution as follows: \n",
    "\n",
    "|$X$ values|marginal distribution| conditional distribution|\n",
    "|:--------:|:-------------------------------------------------:|:-----------------------:|\n",
    "| discrete | $$p_X(x)=\\sum_{y\\in{\\mathcal{X}_Y}}p_{X,Y}(x,y)$$ | $$ p_{Y\\|X}(y\\|x) = \\frac{p_{X,Y}(x,y)}{p_X(x)} $$ |\n",
    "| continuous | $$f_X(x)=\\int_{\\mathcal{X_Y}}f_{X,Y}(x,y)dy$$ | $$ f_{Y\\|X}(y\\|x) = \\frac{f_{X,Y}(x,y)}{f_X(x)} $$ |\n",
    "\n",
    "These definitions easily extend to the multivariate case.\n",
    "\n",
    "\n",
    "Two *r.v.*s $X,Y$ are **independent** if and only if their joint probability equals the product of the marginal probabilities\n",
    "\n",
    "$$f_{X,Y}(x,y)=f_X(x)f_Y(y).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: marginal and conditional from the joint**\n",
    "\n",
    "Let $X$ and $Y$ be two discrete random variables with joint probability distribution\n",
    "$$\n",
    "p(x,y) = \\frac{1}{21}(x+y)\n",
    "$$\n",
    "for $x=1,2,3$ and $y=1,2$.\n",
    "\n",
    "The marginal distribution of $X$ is:\n",
    "$$\n",
    "p(x) = \\sum_{y=1}^2 p(x,y) = \\sum_{y=1}^2 \\frac{1}{21}(x+y) = \\frac{1}{21}(2x+3)\n",
    "$$\n",
    "for $x=1,2,3$.\n",
    "\n",
    "The conditional distribution of $Y$ given $X=1$ is:\n",
    "\n",
    "$$\n",
    "p(y|1)=\\frac{p(1,y)}{p(1)}= \\frac{\\frac{1}{21}(1+y)}{\\frac{5}{21}} = \\frac{1}{5}(1+y)\n",
    "$$\n",
    "\n",
    "for $y=1,2$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation\n",
    "Given two random variables $X$ and $Y$, it is common practice to indicate their density/probability as $p(x)$ and $p(y)$ instead of $p_X(x)$ and $p_Y(y)$. So the density/probability function is somewhat implicit.\n",
    "\n",
    "In other words, the meaning of $p$ depends on the context, in particular the variable name used as argument. \n",
    "For example we can have $p(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\big(\\frac{x-\\mu}{\\sigma}\\big)^2}$ and $p(y) = \\lambda e^{-\\lambda y}$ without contradiction.\n",
    "\n",
    "The letter $p$ indicates a different mass/density function depending on the variable name used as its argument.\n",
    "\n",
    "The same holds for conditional probabilities $p_{X|Y}(x|y) \\rightarrow p(x|y)$, joint distributions $p_{X,Y}(x,y) \\rightarrow p(x,y)$ and in many other cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance and correlation\n",
    "\n",
    "**Covariance** measures the common variation of $X$ and $Y$. \n",
    "It is defined as $\\text{cov}(X,Y)=\\mathbb{E}[(X-\\mathbb{E}[X])(Y-\\mathbb{E}[Y])] = \\mathbb{E}[XY]-\\mathbb{E}[X]\\mathbb{E}[Y].$\n",
    "\n",
    "The covariance of a random variable with itself is called **variance**: $\\text{var}(X)=\\text{cov}(X,X)=\\mathbb{E}[X-\\mathbb{E}(X)^2]$.\n",
    "\n",
    "The **correlation coefficient** between $X$ and $Y$ is the normalized covariance: $\\displaystyle{\\rho=\\frac{\\text{cov}(X,Y)}{\\sqrt{\\text{var}(X)\\text{var}(Y)}}}$. \n",
    "\n",
    "The two variables are said to be *perfectly correlated* when $\\rho=1$ and *anti-correlated* when $\\rho=-1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independence\n",
    "\n",
    "Two random variables $X$ and $Y$ are **independent** if and only if\n",
    "\n",
    "$$\n",
    "p(x,y) = p(x)p(y)\n",
    "$$\n",
    "that is equivalent to\n",
    "$$\n",
    "p(y|x) = p(y)\n",
    "$$\n",
    "$$\n",
    "p(x|y) = p(x)\n",
    "$$\n",
    "\n",
    "Two random variables $X$ and $Y$ are **conditionally independent** given $Z$ if and only if \n",
    "\n",
    "$$p(x|y,z) = p(x|z) \\ \\ \\forall \\ x,y,z$$\n",
    "\n",
    "In other words, knowing $Y$ doesn't add any more information on $X$ if we already know $Z$.\n",
    "\n",
    "An alternative definition is $p(x,y|z) = p(y|z)p(x|z)$. In other words, $X$ and $Y$ are independent events given $Z$.\n",
    "\n",
    "\n",
    "### Law of total probability\n",
    "\n",
    "Let $\\{B_n\\}_{n\\in I}$ be a partition of the sample space, then for any event $A$ in the same probability space:\n",
    "\n",
    "$$P(A) = \\sum_{n\\in I} P(A,B_n) = \\sum_{n \\in I}P(A|B_n)P(B_n).$$\n",
    "\n",
    "In other words, one can compute the probability of an event $A$ by conditioning on all the possible cases belonging to a partition of the sample space.\n",
    "\n",
    "### Bayes' theorem\n",
    "Let $X$ and $Y$ be two random variables with joint probability distribution $p(x,y)$\n",
    "\n",
    "$$p(x | y) = \\frac{p(y | x)p(x)}{p(y)}$$\n",
    "\n",
    "Notice that this is just a trivial consequence of the definition of conditional probability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy\n",
    "The entropy of a distribution is a measure of uncertainty of that distribution, it's related to the amount of information that you know on the random variable $X$ before observing it:\n",
    "$$\\text{H}(p) = -\\mathbb{E}_{x \\in \\mathcal{X}} [\\log{p(x)}]$$\n",
    "\n",
    "### Kullback–Leibler divergence\n",
    "Kullback–Leibler (KL) divergence is a measure of difference between two distributions:\n",
    "\n",
    "$$ D_{KL}(\\ p \\ ||\\  q\\ ) := \\mathbb{E}_{x \\sim p(x)}[\\log{p(x)} - \\log{q(x)}]$$\n",
    "\n",
    "A simple interpretation of the KL divergence of $p$ from $q$ is the expected excess surprise from using $q$ as a model when the actual distribution is $p$.\n",
    "\n",
    "An important property is that $D_{KL}(\\ p \\ ||\\  q\\ ) \\geq 0$ and $D_{KL}(p || q) = 0  \\iff p=q$.\n",
    "\n",
    "However it is not a distance, since in general $D_{KL}(p || q) \\neq D_{KL}(q || p)$\n",
    "\n",
    "### Mutual Information\n",
    "The mutual information between two random variables $X$ and $Y$ is a measure of dependence:\n",
    "$$\\text{I}(X,Y) = D_{KL}(\\ p(x,y)\\ ||\\ p(x)p(y)\\ )$$\n",
    "\n",
    "It measures how much information one variable carries about the other, by comparing the joint distribution with the product of the marginals.\n",
    "\n",
    "Notice that $ \\text{I}(X,Y) = \\text{H}(X) + \\text{H}(Y) - \\text{H}(X,Y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random variables: more formal definition\n",
    "\n",
    "**Measurable space** $(\\Omega,\\mathcal{F})$:\n",
    "\n",
    "- $\\Omega$ is a set;\n",
    "- $\\mathcal{F}$ is a $\\sigma$-algebra on $\\Omega$, i.e. $\\mathcal{F}$:\n",
    "    - contains $\\emptyset, \\Omega$;\n",
    "    - is closed under complementary sets;\n",
    "    - is closed under countable unions.\n",
    "\n",
    "\n",
    "**Measurable function** $f$:\n",
    "\n",
    "- $f:(\\Omega_1,\\mathcal{F}_1)\\rightarrow (\\Omega_2,\\mathcal{F}_2)$;\n",
    "- The pre-image $f^{-1}(E)$, $\\forall$ measurable set $E\\in\\mathcal{F}_2$, is measurable (i.e. $f^{-1}(E)\\in\\mathcal{F}_1$).\n",
    "\n",
    "\n",
    "**Probability measure** $P$ on $(\\Omega,\\mathcal{F})$:\n",
    "\n",
    "- $P:\\mathcal{F}\\rightarrow [0,1]$;\n",
    "- $P$ is countably additive on pairwise disjoint sets;\n",
    "- $P(\\emptyset)=0$ and $P(\\Omega)=1$.\n",
    "\n",
    "\n",
    "**Random variable** $X$:\n",
    "\n",
    "- $(\\Omega,\\mathcal{F},P)$ probability space;\n",
    "- $(\\mathcal{X},\\mathcal{A})$ measurable space;\n",
    "- Measurable $X:(\\Omega,\\mathcal{F},P)\\rightarrow (\\mathcal{X},\\mathcal{A})$.\n",
    "\n",
    "$X$ induces the push-forward probability measure $\\mu$ on $\\mathcal{X}$: $\\mu(A):=X_*P(A)=P(X\\in A) := P(X^{-1}(A))$ for any $A\\in\\mathcal{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "-  [J. Jacod, P. Protter, \"Probability Essentials\"](https://zero.sci-hub.ru/6098/787f72eac157546be3d98fcc129b8ba6/jacod2004.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
