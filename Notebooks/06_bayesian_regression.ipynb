{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65165768",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/DavideScassola/PML2024/blob/main/./Notebooks/06_bayesian_LR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 6: Bayesian Linear Regression & Classification\n",
    "\n",
    "Probabilistic Machine Learning -- Spring 2024, UniTS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we explore linear regression and classification from a Bayesian point of view. In both cases, we start implementing the classical formulation and then we switch to the Bayesian version. We can then compare the methods on a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "The first model that we implement is the well known linear regression model and we use the \"linear algebra view\" that we studied during the lessons.\n",
    "\n",
    "As a case study, we use the [Kaggle dataset](https://www.kaggle.com/datasets/budincsevity/szeged-weather/data) about Weather in Szeged (Hungary) between 2006 and 2016 and we try to predict the apparent temperature given some predictors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherHistory = pd.read_csv('https://raw.githubusercontent.com/DavideScassola/PML2024/main/Notebooks/data/weatherHistory.csv')\n",
    "weatherHistory.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherHistory.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's reduce the size of the dataset by considering one row for each day (corresponding to 16:00), only in 2015\n",
    "#Training: April, Test: May\n",
    "weatherHistory['Formatted Date'] = pd.to_datetime(weatherHistory['Formatted Date'], utc=True)\n",
    "data = weatherHistory[((weatherHistory['Formatted Date'].dt.hour == 16) & (weatherHistory['Formatted Date'].dt.year == 2014) & (weatherHistory['Formatted Date'].dt.month == 4))].reset_index(drop=True)\n",
    "data.info()\n",
    "data_test = weatherHistory[((weatherHistory['Formatted Date'].dt.hour == 16) & (weatherHistory['Formatted Date'].dt.year == 2014) & (weatherHistory['Formatted Date'].dt.month == 5))].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data[\"Apparent Temperature (C)\"], data[\"Humidity\"], label = \"train\")\n",
    "plt.scatter(data_test[\"Apparent Temperature (C)\"], data_test[\"Humidity\"], label = \"test\")\n",
    "\n",
    "plt.xlabel(\"Humidity\")\n",
    "plt.ylabel(\"Apparent Temperature (C)\")\n",
    "plt.title(\"Apparent Temperature vs Humidity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform the linear regression, we follow the maximum likelihood approach:\n",
    "$$\\theta_{ML} = \\text{argmax } p(\\underline{y} | \\underline{x}, \\theta)$$\n",
    "\n",
    "and we define our parametric model as:\n",
    "$$p(y|x, \\theta) = \\mathcal{N}(y|f(x,w), \\beta^{-1})$$\n",
    "\n",
    "where \n",
    "\n",
    "$$f(x,w) = w_0 \\phi_0(x) + \\ldots + w_{M-1}\\phi_{M-1}(x)$$\n",
    "\n",
    "with $\\phi$ basis functions.\n",
    "\n",
    "The weighs can be computed by:\n",
    "\n",
    "$$w_M = \\left( \\Phi^T \\Phi \\right)^{-1} \\Phi^T \\underline{y}$$\n",
    "\n",
    "where $\\Phi$ is the design matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_basis(x):\n",
    "    return x\n",
    "\n",
    "def linear_regression(x,y,basis_function):\n",
    "    N = len(y)\n",
    "    train_features = basis_function(x)\n",
    "    X = np.hstack((np.ones((N,1)), train_features))\n",
    "\n",
    "    # TODO\n",
    "    return\n",
    "\n",
    "def predict_linear_regression(x,N,w,basis_function):\n",
    "    train_features = basis_function(x)\n",
    "    X = np.hstack((np.ones((N,1)), train_features))\n",
    "    return np.dot(X, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform the linear regression on the dataset and compute:\n",
    "\n",
    "- Mean Squared Error (MSE): $$\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2$$\n",
    "- Log Likelihood: $$\\sum_i -\\frac{1}{2} \\log(2\\pi\\sigma_i^2) - \\frac{(y_i - \\mu_i)^2}{2\\sigma_i^2}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in this case x and y are row vectors, so we need to convert them to column vectors\n",
    "x = data[\"Humidity\"].to_numpy().reshape(-1,1)\n",
    "y = data[\"Apparent Temperature (C)\"].to_numpy().reshape(-1,1)\n",
    "\n",
    "beta = 0.01\n",
    "print(np.sqrt(1/beta))\n",
    "\n",
    "y_pred_linear, w = linear_regression(x,y,linear_basis)\n",
    "\n",
    "plt.scatter(data[\"Humidity\"], data[\"Apparent Temperature (C)\"], label = 'Data')\n",
    "plt.scatter(data[\"Humidity\"].to_numpy(), y_pred_linear, label = 'Predictions')\n",
    "plt.xlabel(\"Humidity\")\n",
    "plt.ylabel(\"Apparent Temperature (C)\")\n",
    "plt.title(\"Linear Regression\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"MSE:\")\n",
    "print(np.mean((y_pred_linear.T-y)**2))\n",
    "print(\"Log Likelihood:\")\n",
    "print(-len(y)/2*np.log(2*np.pi)-len(y)/2*np.log(1/beta) - 1/2*beta*np.sum((y_pred_linear.T-y)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see now on the test set\n",
    "\n",
    "x_test = data_test[\"Humidity\"].to_numpy().reshape(-1,1)\n",
    "y_test = data_test[\"Apparent Temperature (C)\"].to_numpy().reshape(-1,1)\n",
    "\n",
    "y_test_pred= predict_linear_regression(x_test,len(x_test),w,linear_basis)\n",
    "\n",
    "plt.scatter(data_test[\"Humidity\"], data_test[\"Apparent Temperature (C)\"], label = 'Data')\n",
    "plt.scatter(data_test[\"Humidity\"].to_numpy(), y_test_pred, label = 'Predictions')\n",
    "plt.xlabel(\"Humidity\")\n",
    "plt.ylabel(\"Apparent Temperature (C)\")\n",
    "plt.title(\"Linear Regression (test data)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"MSE:\")\n",
    "print(np.mean((y_test_pred.T-y_test)**2))\n",
    "print(\"Log Likelihood:\")\n",
    "print(-len(y_test)/2*np.log(2*np.pi*1/beta) - 1/2*np.sum((y_test_pred.T-y_test)**2*beta))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's try to change the basis function\n",
    "\n",
    "def square_basis(x):\n",
    "    return  x**2\n",
    "\n",
    "y_pred, w = linear_regression(x,y,square_basis)\n",
    "\n",
    "plt.scatter(data[\"Humidity\"], data[\"Apparent Temperature (C)\"], label = 'Data')\n",
    "plt.scatter(data[\"Humidity\"].to_numpy(), y_pred, label = 'Predictions')\n",
    "plt.xlabel(\"Humidity\")\n",
    "plt.ylabel(\"Apparent Temperature (C)\")\n",
    "plt.title(\"Linear Regression\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"MSE:\")\n",
    "print(np.mean((y_pred.T-y)**2))\n",
    "print(\"Log Likelihood:\")\n",
    "print(-len(y)/2*np.log(2*np.pi)-len(y)/2*np.log(1/beta) - 1/2*beta*np.sum((y_pred.T-y)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can also consider more predictors\n",
    "\n",
    "x = data[[\"Temperature (C)\", \"Humidity\", \"Wind Speed (km/h)\"]].to_numpy()\n",
    "y = data[\"Apparent Temperature (C)\"].to_numpy().reshape(-1,1)\n",
    "\n",
    "y_pred, w = linear_regression(x,y,linear_basis)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "scatter = ax.scatter(data[\"Temperature (C)\"], data[\"Humidity\"], s = data[\"Wind Speed (km/h)\"]*2, c = y_pred)\n",
    "legend1 = ax.legend(*scatter.legend_elements(), loc = 'upper right', title=\"Predicted App. Temp.\")\n",
    "ax.add_artist(legend1)\n",
    "handles, labels = scatter.legend_elements(prop=\"sizes\", alpha=0.6)\n",
    "legend2 = ax.legend(handles, labels, loc = 'lower left', title=\"Wind Speed\")\n",
    "\n",
    "plt.title(\"Linear Regression with more predictors\")\n",
    "plt.ylabel(\"Humidity\")\n",
    "plt.xlabel(\"Temperature (C)\")\n",
    "plt.show()\n",
    "\n",
    "print(\"MSE:\")\n",
    "print(np.mean((y_pred.T-y)**2))\n",
    "print(\"Log Likelihood:\")\n",
    "print(-len(y)/2*np.log(2*np.pi)-len(y)/2*np.log(1/beta) - 1/2*beta*np.sum((y_pred.T-y)**2))\n",
    "print(\"Weights:\")\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Linear Regression\n",
    "\n",
    "We now consider a prior distribution on the weights, which in this case is:\n",
    "\n",
    "$$p(w|\\alpha) = \\mathcal{N}(w| 0, \\alpha^{-1} I)$$\n",
    "\n",
    "So, we can use Bayes theorem to compute the posterior:\n",
    "\n",
    "$$    p(w| \\underline{x}, \\underline{y}, \\alpha, \\beta) = \\frac{p(\\underline{y}| \\underline{x}, w, \\beta) p(w|\\alpha)}{p(\\underline{y}| \\underline{x}, \\alpha, \\beta)}$$\n",
    "\n",
    "and notice that it is a Gaussian distribution:\n",
    "\n",
    "$$p(w|\\underline{x}, \\underline{y}, \\alpha, \\beta) = \\mathcal{N} (w | m_N, S_N)$$\n",
    "\n",
    "where \n",
    "\n",
    "$$m_N = \\beta S_N \\Phi^T \\underline{y}$$\n",
    "$$S_N^{-1} = \\alpha I + \\beta \\Phi^T \\Phi$$\n",
    "\n",
    "Once we have the posterior distribution, we can calculate the predictive distribution as:\n",
    "\n",
    "$$p(y|x, \\underline{x} , \\underline{y}, \\alpha, \\beta)  = \\mathcal{N} \\left(y| m_N^T \\phi(x), \\sigma_N^2(x) \\right)$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\\sigma_N^2(x) = \\frac{1}{\\beta} + \\phi^T(x) S_N \\phi(x)$$\n",
    "\n",
    "For simplicity and for visualizing the result, we test on a single predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bayesian Regression with alpha and beta fixed and a gaussian prior\n",
    "\n",
    "def bayesian_linear_regression(x,y,basis_function, alpha, beta):\n",
    "    N = len(y)\n",
    "    train_features = basis_function(x)\n",
    "    X = np.hstack((np.ones((N,1)), train_features))\n",
    "    # TODO\n",
    "    return \n",
    "\n",
    "def predict_bayesian_linear_regression(mN, SN, N, x, basis_function, beta):\n",
    "    train_features = basis_function(x)\n",
    "    X = np.hstack((np.ones((N,1)), train_features))\n",
    "    # TODO\n",
    "    return \n",
    "\n",
    "def plot_bayesian_LR(mN, SN, N, x, basis_function, beta):\n",
    "    mean, sigma = predict_bayesian_linear_regression(mN, SN, N, x, basis_function, beta)\n",
    "    plt.plot(x.T[0],mean)\n",
    "    plt.fill_between(x.T[0], mean-sigma, mean+sigma, alpha=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[\"Humidity\"].to_numpy().reshape(-1,1)\n",
    "y = data[\"Apparent Temperature (C)\"].to_numpy().reshape(-1,1)\n",
    "\n",
    "x_test = data_test[\"Humidity\"].to_numpy().reshape(-1,1)\n",
    "y_test = data_test[\"Apparent Temperature (C)\"].to_numpy().reshape(-1,1)\n",
    "\n",
    "alpha = 0.002\n",
    "\n",
    "y_pred_bayes, sigma_pred, mN, SN = bayesian_linear_regression(x,y,linear_basis, alpha, beta)\n",
    "\n",
    "print(\"MSE (training set):\")\n",
    "print(np.mean((y_pred_bayes-y)**2))\n",
    "print(\"Log Likelihood (training set):\")\n",
    "print(-1/2*np.sum(np.log(2*np.pi*sigma_pred**2)) - 1/2*np.sum(((y_pred_bayes-y)**2)/(sigma_pred**2)))\n",
    "\n",
    "mean, sigma = predict_bayesian_linear_regression(mN, SN, len(x_test), x_test, linear_basis, beta)\n",
    "print(\"MSE (test set):\")\n",
    "print(np.mean((mean-y_test)**2))\n",
    "print(\"Log Likelihood (test set):\")\n",
    "print(-1/2*np.sum(np.log(2*np.pi*sigma**2)) - 1/2*np.sum(((mean-y_test)**2)/(sigma**2)))\n",
    "\n",
    "#FOR 1D features, we can plot the regression\n",
    "xrange=np.linspace(0, 1, 100).reshape(-1,1)\n",
    "plot_bayesian_LR(mN, SN, len(xrange), xrange, linear_basis, beta)\n",
    "\n",
    "plt.scatter(data[\"Humidity\"], data[\"Apparent Temperature (C)\"], label='Data training')\n",
    "plt.scatter(data_test[\"Humidity\"], data_test[\"Apparent Temperature (C)\"], label='Data test')\n",
    "\n",
    "plt.xlabel(\"Humidity\")\n",
    "plt.ylabel(\"Apparent Temperature (C)\")\n",
    "plt.title(\"Bayesian Linear Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given observations $(\\textbf{x}_i, y_i)$ with $x_i \\in \\mathbb{R}^d$ and $y_i \\in \\{0,1\\}$ we want to model $p(y | \\textbf{x})$.\n",
    "\n",
    "**Notation**\n",
    "\n",
    "$\\textbf{X}$: the dataset of input features $\\textbf{x}_i$\n",
    "\n",
    "$\\textbf{Y}$: the dataset of targets $y_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "\n",
    "\n",
    "# URL of the file to download\n",
    "url = 'https://raw.githubusercontent.com/DavideScassola/PML2024/main/Notebooks/data/logistic_regression_data.npz'\n",
    "\n",
    "# Send a HTTP request to the URL of the file\n",
    "response = requests.get(url)\n",
    "\n",
    "# Ensure the 'data/' directory exists\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# Write the file\n",
    "with open('data/logistic_regression_data.npz', 'wb') as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "# loading the dataset\n",
    "X = np.load('data/logistic_regression_data.npz')['X']\n",
    "y = np.load('data/logistic_regression_data.npz')['y']\n",
    "\n",
    "\n",
    "# let's split it into train and test set and normalize it\n",
    "def split_and_standardize(*, X, y, ratio=0.8):\n",
    "    \n",
    "    # Splitting train and test sets\n",
    "    cut = int(ratio * len(y))\n",
    "    X_train = X[:cut]\n",
    "    y_train = y[:cut]\n",
    "    \n",
    "    X_test = X[cut:]\n",
    "    y_test = y[cut:]\n",
    "    \n",
    "    # Standardizing data\n",
    "    x_mean = X_train.mean(axis=0)\n",
    "    x_std = X_train.std(axis=0)\n",
    "    \n",
    "    X_train = (X_train - x_mean) / x_std\n",
    "    X_test = (X_test - x_mean) / x_std\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = split_and_standardize(X=X, y=y, ratio=0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_ = plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In logistic regression $p(y | \\textbf{x})$ is modelled as:\n",
    "\n",
    "$$p_{\\textbf{w}}(y |  \\textbf{x}) = Bernoulli(y; f(\\textbf{w}^{T} \\textbf{x})) $$\n",
    "\n",
    "where $f : \\mathbb{R} \\rightarrow [0,1]$ is usually chosen to be the Logit $ \\left( ({1 + e^{-k}})^{-1} \\right)$ or the Probit.\n",
    "\n",
    "Why $\\textbf{w}^{T} \\textbf{x}$ and not $\\textbf{w}^{T} \\textbf{x} + h$? For simplicity we prefer to avoid explicitly modelling the bias and we equivalently add a \"1\" as a feature to each data point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Add the \"bias feature\"\n",
    "X_train = np.hstack([np.ones((X_train.shape[0], 1)), X_train])\n",
    "X_test = np.hstack([np.ones((X_test.shape[0], 1)), X_test])\n",
    "\n",
    "\n",
    "def logit(x: np.ndarray) -> np.ndarray:\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def likelihood(X: np.ndarray, y: np.ndarray, w: np.ndarray) -> float:\n",
    "    p = logit(X @ w)  # (n, d) @ (d, )\n",
    "    return np.where(y == 1, p, 1 - p).prod()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum likelihood fitting (frequentist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit our model $p_{\\textbf{w}}(y |  \\textbf{x})$ by finding parameters $w$ that maximize the likelihood according to our data. For numerical reasons we equivalently maximize the log-likelihood:\n",
    "\n",
    "$$ \\log{p_{\\textbf{w}}(Y |  \\textbf{X})} =  \\log{ \\prod_i {        p_{\\textbf{w}}(y_i |  \\textbf{x}_i)             }} = \\sum_i \\log{p_{\\textbf{w}}(y_i |  \\textbf{x}_i)} = \\sum_i \\log{Bernoulli(y_i; f(\\textbf{w}^{T} \\textbf{x}_i))}$$\n",
    "\n",
    "We can find $w_{ML} = \\argmax_{\\textbf{w}}  \\log{p_{\\textbf{w}}(Y |  \\textbf{X})}$ using numerical optimization, in particular we can use gradient descent:\n",
    "\n",
    "$$ w_{n+1} = w_{n} + \\eta \\nabla_{\\textbf{w}} \\log{p_{\\textbf{w}}(Y |  \\textbf{X})} $$\n",
    "\n",
    "where $\\eta$ is the learning rate. How do we compute the gradient? We can compute it analyticallly, but we can also use automatic differentiation with `pytorch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e8d1a25203047ce9aef468ec83a167c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=0.09124\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def log_likelihood(*, X: torch.Tensor, y: torch.Tensor, w: torch.Tensor) -> torch.Tensor:\n",
    "    p = torch.sigmoid(X @ w)\n",
    "    return torch.where(y == 1, torch.log(p + 1e-8), torch.log(1 - p + 1e-8)).sum()\n",
    "\n",
    "\n",
    "def gradient_descent_optimization(*, loss_function, lr: float, n_iter: int, initial_guess: np.ndarray) -> np.ndarray:\n",
    "    w = torch.tensor(initial_guess, dtype=torch.float32, requires_grad=True)\n",
    "    for _ in tqdm(range(n_iter)):\n",
    "        loss = loss_function(w)\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            w -= lr * w.grad\n",
    "        w.grad = None\n",
    "    print(f\"loss={ loss.item():.4g}\")\n",
    "    return w.cpu().detach().numpy()\n",
    "\n",
    "X_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "loss_function = lambda w: -log_likelihood(X=X_tensor,\n",
    "                                          y=y_tensor,\n",
    "                                          w=w)\n",
    "\n",
    "w_mle = gradient_descent_optimization(loss_function=loss_function,\n",
    "                                      initial_guess=np.zeros(X_train.shape[1]),\n",
    "                                      lr=1e-2,\n",
    "                                      n_iter=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: \n",
      " [[3 0]\n",
      " [0 1]]\n",
      "mean cross entropy: 0.0228\n"
     ]
    }
   ],
   "source": [
    "# Veryfing the results\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def predict(*, x: np.ndarray, w: np.ndarray) -> np.ndarray:\n",
    "    return logit(x @ w)\n",
    "\n",
    "def mean_cross_entropy(*, y: np.ndarray, pred: np.ndarray) -> float:\n",
    "    return -np.where(y, np.log(pred + 1e-8), np.log(1 - pred + 1e-8)).mean()\n",
    "\n",
    "frequentist_predictions = predict(x=X_train, w=w_mle)\n",
    "print(\"confusion matrix: \\n\", confusion_matrix(y_train, frequentist_predictions > 0.5))\n",
    "print(f\"mean cross entropy: {mean_cross_entropy(y=y_train, pred=frequentist_predictions):.3g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Bayesian Inference, we want to compute the **posterior distribution**:\n",
    "$$p(\\textbf{w} | \\mathbf{X}, Y) \\propto p(Y | \\textbf{w}, \\mathbf{X})p(\\textbf{w})$$\n",
    "\n",
    "\n",
    "where $p(\\textbf{w})$ is the prior, and the **predictive distribution**:\n",
    "$$p(y | \\mathbf{x}, \\mathbf{X}, Y) = \\int p(y | \\mathbf{x}, \\textbf{w}) p(\\textbf{w} | \\mathbf{X}, Y) d\\textbf{w}$$\n",
    "\n",
    "notice that $\\mathbf{X}$ is always in the conditioning, we are not interested in its distribution!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Posterior distribution\n",
    "\n",
    "For the posterior $p(\\textbf{w} | \\mathbf{X}, Y) \\propto p(Y | \\textbf{w}, \\mathbf{X})p(\\textbf{w})$ we resort to the **Laplace Approximation**:\n",
    "\n",
    "$$p(\\textbf{w} | \\mathbf{X}, Y) \\approx \\mathcal{N}(\\textbf{w}; \\mu_D, \\Sigma_D)$$\n",
    "\n",
    "where $\\mu_D = \\textbf{w}_{MAP}$ and $\\Sigma_D =  \\left( - \\nabla_w \\nabla_w \\log{p(Y | \\textbf{w}_{MAP}, \\mathbf{X})} \\right)^{-1}$\n",
    "\n",
    "where $\\textbf{w}_{MAP} = \\argmax_{\\textbf{w}} \\left[ p(\\textbf{w} | \\mathbf{X}, Y) \\right]$ is the **maximum a posteriori**, equivalently $\\textbf{w}_{MAP} = \\argmax_{\\textbf{w}}  \\left[ \\log{p(Y | \\textbf{w}, \\mathbf{X})} + \\log{p(\\textbf{w})} \\right]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can assume a Gaussian prior for the weights, with zero mean and a certain diagonal covariance matrix\n",
    "from torch.distributions import Normal\n",
    "\n",
    "PRIOR_STD = 100\n",
    "\n",
    "# Define the prior log density (it's a Gaussian with mean=0 and std=PRIOR_STD)\n",
    "def log_prior(w: torch.Tensor) -> torch.Tensor:\n",
    "    return ...  # TODO\n",
    "\n",
    "# Define the log density of the unnormalized posterior\n",
    "def log_unnormalized_posterior(*, X: torch.Tensor, y: torch.Tensor, w: torch.Tensor) -> torch.Tensor:\n",
    "    return ... # TODO\n",
    "\n",
    "# Find the maximum a posteriori (MAP) estimate\n",
    "w_map = ... # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the Hessian (second derivative) of any function f with respect to input x\n",
    "def compute_hessian(*, f, x: np.ndarray) -> np.ndarray:\n",
    "    x_tensor = torch.tensor(x, dtype=torch.float32, requires_grad = True)\n",
    "    grad = torch.autograd.grad(f(x_tensor), x_tensor, create_graph=True)[0]\n",
    "    hessian = torch.zeros((x_tensor.numel(), x_tensor.numel()))\n",
    "    for i in range(x_tensor.numel()):\n",
    "        hessian[i] = torch.autograd.grad(grad[i], x_tensor, retain_graph=True)[0]\n",
    "    x_tensor.requires_grad = False\n",
    "    return hessian.numpy()\n",
    "\n",
    "# Find the posterior mean and covariance according to the Laplace Approximation\n",
    "posterior_mean = ... # TODO\n",
    "posterior_cov = ... # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictive distribution\n",
    "\n",
    "The approximation for the **predictive distribution** follows from the Laplace approximation of the posterior approximation (see lecture's notes for more details):\n",
    "$$p(y | \\mathbf{x}, \\mathbf{X}, Y) \\approx f\\left(\\kappa\\left(\\sigma_a^2\\right) \\mu_a\\right)$$\n",
    "\n",
    "where $\\kappa\\left(\\sigma_a^2\\right)=\\left(1+\\pi \\frac{\\sigma_a^2}{8}\\right)^{-\\frac{1}{2}}$ and $\\mu_a$ and $\\sigma_a^2$ are the mean and variance of the linear combination of the gaussian posterior with the observation $\\mathbf{x}$, so $\\mu_a = \\mathbf{\\mu}_{D}^T \\mathbf{x}$ and $\\sigma_a^2 = \\mathbf{x}^T \\Sigma_{D} \\mathbf{x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the predictive distribution given input x and the Laplace approximation\n",
    "def laplace_predictive_distribution(*, x: np.ndarray, posterior_mean: np.ndarray, posterior_cov: np.ndarray) -> np.ndarray: \n",
    "    return ... # TODO\n",
    "\n",
    "bayesian_predictions = laplace_predictive_distribution(x=X_test, posterior_mean=w_map, posterior_cov=posterior_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the performace (accuracy and cross entropy) of the frequentist and Bayesian models on the test set\n",
    "... # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the following code to plot the data and the predicted values of the Bayesian model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (8,4)\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "\n",
    "def plot_train_data(X, y):\n",
    "    plt.scatter(X[y==1][:,1], X_train[y==1][:,2], c = 'red', marker='x', label='1')\n",
    "    plt.scatter(X[y==0][:,1], X_train[y==0][:,2], c = 'red', marker='o', label='0')\n",
    "    plt.legend()\n",
    "\n",
    "def binary_crossentropy(*, true: np.ndarray, pred: np.ndarray):\n",
    "    return -np.where(true, np.log(pred + 1e-8), np.log(1 - pred + 1e-8)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_test[:,1], X_test[:,2], c = y_test, alpha = 0.8)\n",
    "plot_train_data(X_train, y_train)\n",
    "_ = plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = laplace_predictive_distribution(x=X_test, posterior_mean=w_map, posterior_cov=posterior_cov)\n",
    "error = binary_crossentropy(true=y_test, pred=pred)\n",
    "plt.scatter(X_test[:,1], X_test[:,2], c = pred, alpha = 0.8)\n",
    "plot_train_data(X_train, y_train)\n",
    "_ = plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict(x=X_test, w=w_map)\n",
    "error = binary_crossentropy(true=y_test, pred=pred)\n",
    "plt.scatter(X_test[:,1], X_test[:,2], c = pred, alpha = 0.8)\n",
    "plot_train_data(X_train, y_train)\n",
    "_ = plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Try to repeat the analysis using more points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Try to repeat the analysis on the following dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
