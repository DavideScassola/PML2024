{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a9934de",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/DavideScassola/PML2024/blob/main/Notebooks/02_numpy_pandas_sklearn/023_scikit_learn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2.3: Introduction to scikit-learn\n",
    "\n",
    "This is a brief tutorial on scikit-learn, crafted from online material, in particular from the [scikit-learn webpage](https://scikit-learn.org/stable/).\n",
    "\n",
    "Scikit-learn is a python package that implements a very wide set of tools useful for machine learning.\n",
    "\n",
    "### What can we do with scikit-learn?\n",
    "\n",
    "* [Data preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing)\n",
    "* [Dimensionality reduction of large datasets](https://scikit-learn.org/stable/modules/decomposition.html#decompositions)\n",
    "* [Supervised learning for both classification and regression](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning)\n",
    "* [Unsupervised learning and clustering](https://scikit-learn.org/stable/unsupervised_learning.html)\n",
    "* [Model selection](https://scikit-learn.org/stable/model_selection.html#model-selection)\n",
    "* [Composing processing steps together as Pipelines](https://scikit-learn.org/stable/modules/compose.html)\n",
    "\n",
    "The main strength the scikit-learn API has is its consistency: [all estimators, transformers, etc share the same main design](https://scikit-learn.org/stable/glossary.html#glossary-estimator-types), enabling very flexible compositions of different machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "Scikit-learn comes with some built-in datasets for use. These are in the module `dataset`.\n",
    "Each datasets contains input data (the `data` key in the dictionary), targets, and input and target names. It may contain also additional information, e.g. raw images in case of image datasets. Pandas dataframes can be used as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the attibutes of digits  ['DESCR', 'data', 'feature_names', 'frame', 'images', 'target', 'target_names']\n",
      "<class 'numpy.ndarray'>\n",
      "(1797, 8, 8)\n",
      "[[ 0.  0. 10. 14.  8.  1.  0.  0.]\n",
      " [ 0.  2. 16. 14.  6.  1.  0.  0.]\n",
      " [ 0.  0. 15. 15.  8. 15.  0.  0.]\n",
      " [ 0.  0.  5. 16. 16. 10.  0.  0.]\n",
      " [ 0.  0. 12. 15. 15. 12.  0.  0.]\n",
      " [ 0.  4. 16.  6.  4. 16.  6.  0.]\n",
      " [ 0.  8. 16. 10.  8. 16.  8.  0.]\n",
      " [ 0.  1.  8. 12. 14. 12.  1.  0.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "print(\"These are the attibutes of digits \", dir(digits))\n",
    "plt.imshow(digits.images[-1], cmap=plt.cm.gray_r) \n",
    "\n",
    "print(type(digits.images[-1]))\n",
    "#these are 8x8 pixels images . \n",
    "print(digits.images.shape)\n",
    "print(digits.images[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0. 10. 14.  8.  1.  0.  0.  0.  2. 16. 14.  6.  1.  0.  0.  0.  0.\n",
      "  15. 15.  8. 15.  0.  0.  0.  0.  5. 16. 16. 10.  0.  0.  0.  0. 12. 15.\n",
      "  15. 12.  0.  0.  0.  4. 16.  6.  4. 16.  6.  0.  0.  8. 16. 10.  8. 16.\n",
      "   8.  0.  0.  1.  8. 12. 14. 12.  1.  0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ..., 16.,  9.,  0.],\n",
       "       ...,\n",
       "       [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "       [ 0.,  0.,  2., ..., 12.,  0.,  0.],\n",
       "       [ 0.,  0., 10., ..., 12.,  1.,  0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can in principle reshape the image to trasnform in into a dataset\n",
    "#this transforms a single input point in a row\n",
    "print(digits.images[-1].reshape((1, -1))) \n",
    "# this transforms the whole input images into a row\n",
    "digits.images.reshape((digits.images.shape[0], -1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  5. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ... 10.  0.  0.]\n",
      " [ 0.  0.  0. ... 16.  9.  0.]\n",
      " ...\n",
      " [ 0.  0.  1. ...  6.  0.  0.]\n",
      " [ 0.  0.  2. ... 12.  0.  0.]\n",
      " [ 0.  0. 10. ... 12.  1.  0.]]\n",
      "['pixel_0_0', 'pixel_0_1', 'pixel_0_2', 'pixel_0_3', 'pixel_0_4', 'pixel_0_5', 'pixel_0_6', 'pixel_0_7', 'pixel_1_0', 'pixel_1_1', 'pixel_1_2', 'pixel_1_3', 'pixel_1_4', 'pixel_1_5', 'pixel_1_6', 'pixel_1_7', 'pixel_2_0', 'pixel_2_1', 'pixel_2_2', 'pixel_2_3', 'pixel_2_4', 'pixel_2_5', 'pixel_2_6', 'pixel_2_7', 'pixel_3_0', 'pixel_3_1', 'pixel_3_2', 'pixel_3_3', 'pixel_3_4', 'pixel_3_5', 'pixel_3_6', 'pixel_3_7', 'pixel_4_0', 'pixel_4_1', 'pixel_4_2', 'pixel_4_3', 'pixel_4_4', 'pixel_4_5', 'pixel_4_6', 'pixel_4_7', 'pixel_5_0', 'pixel_5_1', 'pixel_5_2', 'pixel_5_3', 'pixel_5_4', 'pixel_5_5', 'pixel_5_6', 'pixel_5_7', 'pixel_6_0', 'pixel_6_1', 'pixel_6_2', 'pixel_6_3', 'pixel_6_4', 'pixel_6_5', 'pixel_6_6', 'pixel_6_7', 'pixel_7_0', 'pixel_7_1', 'pixel_7_2', 'pixel_7_3', 'pixel_7_4', 'pixel_7_5', 'pixel_7_6', 'pixel_7_7']\n",
      "[0 1 2 ... 8 9 8]\n"
     ]
    }
   ],
   "source": [
    "#though the dataset is already prepared!\n",
    "print(digits.data)\n",
    "print(digits.feature_names)\n",
    "print(digits.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel_0_0</th>\n",
       "      <th>pixel_0_1</th>\n",
       "      <th>pixel_0_2</th>\n",
       "      <th>pixel_0_3</th>\n",
       "      <th>pixel_0_4</th>\n",
       "      <th>pixel_0_5</th>\n",
       "      <th>pixel_0_6</th>\n",
       "      <th>pixel_0_7</th>\n",
       "      <th>pixel_1_0</th>\n",
       "      <th>pixel_1_1</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel_6_7</th>\n",
       "      <th>pixel_7_0</th>\n",
       "      <th>pixel_7_1</th>\n",
       "      <th>pixel_7_2</th>\n",
       "      <th>pixel_7_3</th>\n",
       "      <th>pixel_7_4</th>\n",
       "      <th>pixel_7_5</th>\n",
       "      <th>pixel_7_6</th>\n",
       "      <th>pixel_7_7</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1792</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1793</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1797 rows Ã— 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pixel_0_0  pixel_0_1  pixel_0_2  pixel_0_3  pixel_0_4  pixel_0_5  \\\n",
       "0           0.0        0.0        5.0       13.0        9.0        1.0   \n",
       "1           0.0        0.0        0.0       12.0       13.0        5.0   \n",
       "2           0.0        0.0        0.0        4.0       15.0       12.0   \n",
       "3           0.0        0.0        7.0       15.0       13.0        1.0   \n",
       "4           0.0        0.0        0.0        1.0       11.0        0.0   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "1792        0.0        0.0        4.0       10.0       13.0        6.0   \n",
       "1793        0.0        0.0        6.0       16.0       13.0       11.0   \n",
       "1794        0.0        0.0        1.0       11.0       15.0        1.0   \n",
       "1795        0.0        0.0        2.0       10.0        7.0        0.0   \n",
       "1796        0.0        0.0       10.0       14.0        8.0        1.0   \n",
       "\n",
       "      pixel_0_6  pixel_0_7  pixel_1_0  pixel_1_1  ...  pixel_6_7  pixel_7_0  \\\n",
       "0           0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "1           0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "2           0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "3           0.0        0.0        0.0        8.0  ...        0.0        0.0   \n",
       "4           0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "1792        0.0        0.0        0.0        1.0  ...        0.0        0.0   \n",
       "1793        1.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "1794        0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "1795        0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "1796        0.0        0.0        0.0        2.0  ...        0.0        0.0   \n",
       "\n",
       "      pixel_7_1  pixel_7_2  pixel_7_3  pixel_7_4  pixel_7_5  pixel_7_6  \\\n",
       "0           0.0        6.0       13.0       10.0        0.0        0.0   \n",
       "1           0.0        0.0       11.0       16.0       10.0        0.0   \n",
       "2           0.0        0.0        3.0       11.0       16.0        9.0   \n",
       "3           0.0        7.0       13.0       13.0        9.0        0.0   \n",
       "4           0.0        0.0        2.0       16.0        4.0        0.0   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "1792        0.0        2.0       14.0       15.0        9.0        0.0   \n",
       "1793        0.0        6.0       16.0       14.0        6.0        0.0   \n",
       "1794        0.0        2.0        9.0       13.0        6.0        0.0   \n",
       "1795        0.0        5.0       12.0       16.0       12.0        0.0   \n",
       "1796        1.0        8.0       12.0       14.0       12.0        1.0   \n",
       "\n",
       "      pixel_7_7  target  \n",
       "0           0.0       0  \n",
       "1           0.0       1  \n",
       "2           0.0       2  \n",
       "3           0.0       3  \n",
       "4           0.0       4  \n",
       "...         ...     ...  \n",
       "1792        0.0       9  \n",
       "1793        0.0       0  \n",
       "1794        0.0       8  \n",
       "1795        0.0       9  \n",
       "1796        0.0       8  \n",
       "\n",
       "[1797 rows x 65 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can also use pandas dataframes:\n",
    "df = pd.DataFrame(data=digits.data,\n",
    "                  columns=digits.feature_names)\n",
    "df['target'] = digits.target\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Operations on  Datasets\n",
    "A cool thing of scikit-learn is that is has functions to perform routine operations on datasets, including: \n",
    "* generating Test and Train datasets. The relevant function is `train_test_split` from the module `model_selection`.\n",
    "* Standardising input data. The function scale of the module `preprocessing` provides a quick and easy way to perform this operation on a single array-like dataset.\n",
    "\n",
    "\n",
    "A very important parameter of `train_test_split` is `stratify`. If left as `None`, `train_test_split` will uniformly randomly select rows to use in the train and test sets. This criteria may produce unbalanced sets (i.e. the label distribution may be different between the train and test sets). For very big datasets, or datasets are well balanced to begin with, this is usually not a problem; but when the datasets are smaller, this can have serious consequences. For example, it may happen that some labels can be completely abscent in the train or test set, and the learnt models will work poorly.\n",
    "\n",
    "If you supply an array to `stratify`, the train and test split is done on the rows corresponding to each of the unique values of the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([178, 182, 177, 183, 181, 182, 181, 179, 174, 180]))\n",
      "---\n",
      "Without stratify\n",
      "original\n",
      "{0: 0.09905397885364496, 1: 0.10127991096271564, 2: 0.09849749582637729, 3: 0.1018363939899833, 4: 0.10072342793544797, 5: 0.10127991096271564, 6: 0.10072342793544797, 7: 0.09961046188091263, 8: 0.09682804674457429, 9: 0.1001669449081803}\n",
      "---\n",
      "train\n",
      "{0: 0.10946196660482375, 1: 0.10111317254174397, 2: 0.09833024118738404, 3: 0.10482374768089053, 4: 0.10946196660482375, 5: 0.0862708719851577, 6: 0.09740259740259741, 7: 0.10575139146567718, 8: 0.08905380333951762, 9: 0.09833024118738404}\n",
      "---\n",
      "test\n",
      "{0: 0.08344923504867872, 1: 0.10152990264255911, 2: 0.09874826147426982, 3: 0.09735744089012517, 4: 0.08762169680111266, 5: 0.12378303198887343, 6: 0.10570236439499305, 7: 0.09040333796940195, 8: 0.10848400556328233, 9: 0.10292072322670376}\n",
      "---\n",
      "\n",
      "Using stratify\n",
      "original\n",
      "{0: 0.09905397885364496, 1: 0.10127991096271564, 2: 0.09849749582637729, 3: 0.1018363939899833, 4: 0.10072342793544797, 5: 0.10127991096271564, 6: 0.10072342793544797, 7: 0.09961046188091263, 8: 0.09682804674457429, 9: 0.1001669449081803}\n",
      "---\n",
      "train\n",
      "{0: 0.09925788497217068, 1: 0.10111317254174397, 2: 0.09833024118738404, 3: 0.10204081632653061, 4: 0.10111317254174397, 5: 0.10111317254174397, 6: 0.10111317254174397, 7: 0.09925788497217068, 8: 0.09647495361781076, 9: 0.10018552875695733}\n",
      "---\n",
      "test\n",
      "{0: 0.09874826147426982, 1: 0.10152990264255911, 2: 0.09874826147426982, 3: 0.10152990264255911, 4: 0.10013908205841446, 5: 0.10152990264255911, 6: 0.10013908205841446, 7: 0.10013908205841446, 8: 0.09735744089012517, 9: 0.10013908205841446}\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# First lets see the label distribution for the train and test sets without stratification.\n",
    "print(np.unique(digits.target, return_counts=True))\n",
    "print('---')\n",
    "digits_X_train, digits_X_test, digits_y_train, digits_y_test = \\\n",
    "    train_test_split(digits.data, digits.target, test_size=0.4, random_state=0)\n",
    "print('Without stratify')\n",
    "a = {'original': digits.target,\n",
    "     'train': digits_y_train,\n",
    "     'test': digits_y_test}\n",
    "for kind, vals in a.items():\n",
    "    print(kind)\n",
    "    print({label: c / len(vals) for label, c in zip(*np.unique(vals, return_counts=True))})\n",
    "    print('---')\n",
    "\n",
    "# Now lets see what happens when we use stratify\n",
    "\n",
    "digits_X_train2, digits_X_test2, digits_y_train2, digits_y_test2 = \\\n",
    "    train_test_split(digits.data, digits.target, test_size=0.4, random_state=0,\n",
    "                     stratify=digits.target)\n",
    "a = {'original': digits.target,\n",
    "     'train': digits_y_train2,\n",
    "     'test': digits_y_test2}\n",
    "print('\\nUsing stratify')\n",
    "for kind, vals in a.items():\n",
    "    print(kind)\n",
    "    print({label: c / len(vals) for label, c in zip(*np.unique(vals, return_counts=True))})\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer API and Preprocessing\n",
    "\n",
    "Scikit-learn has some API/ interfaces which are very useful to automate the learning process.\n",
    "As data preprocessing has an important role in learning - typically dataset are standardised or normalised in some way - there several classes implementing the Transformer API. \n",
    "\n",
    "The two relevant methods of a transformer object are `fit()` and `transform()`. The first one is used to train the transformer, e.g. define the standardisation for each feature, the second one to apply the transformation to novel data. Transformer objects can be trained on the train datasets and then applied to test datasets, hence they should be the methods of choice.  \n",
    "\n",
    "\n",
    "Data preprocessing is usually performed through the `Scaler` objects, which are of several types:\n",
    "* `StandardScaler` performs classic standardisation;\n",
    "* `MinMaxScaler` normalises data into [0,1], where 0 is the minimum and 1 is the maximum of each feature, but different ranges can be specified as well;\n",
    "* `MaxAbsScaler` divides each feature for the absolute value, hence data is scaled in [-1,1];\n",
    "* `RobustScaler` deals better with data with outliers, as it removes the median and scales the data according to the quantile range;\n",
    "\n",
    "Other methods perform non-linear transformations:\n",
    "* `QuantileTransformer` performs a non-linear transformation using quantiles. Better for outliers, but being non-linear, it breaks the correlation. \n",
    "* `Normalizer` scales individual samples to have unit norm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class MinMaxScaler in module sklearn.preprocessing._data:\n",
      "\n",
      "class MinMaxScaler(sklearn.base.OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      " |  MinMaxScaler(feature_range=(0, 1), *, copy=True, clip=False)\n",
      " |  \n",
      " |  Transform features by scaling each feature to a given range.\n",
      " |  \n",
      " |  This estimator scales and translates each feature individually such\n",
      " |  that it is in the given range on the training set, e.g. between\n",
      " |  zero and one.\n",
      " |  \n",
      " |  The transformation is given by::\n",
      " |  \n",
      " |      X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
      " |      X_scaled = X_std * (max - min) + min\n",
      " |  \n",
      " |  where min, max = feature_range.\n",
      " |  \n",
      " |  This transformation is often used as an alternative to zero mean,\n",
      " |  unit variance scaling.\n",
      " |  \n",
      " |  `MinMaxScaler` doesn't reduce the effect of outliers, but it linearily\n",
      " |  scales them down into a fixed range, where the largest occuring data point\n",
      " |  corresponds to the maximum value and the smallest one corresponds to the\n",
      " |  minimum value. For an example visualization, refer to :ref:`Compare\n",
      " |  MinMaxScaler with other scalers <plot_all_scaling_minmax_scaler_section>`.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <preprocessing_scaler>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  feature_range : tuple (min, max), default=(0, 1)\n",
      " |      Desired range of transformed data.\n",
      " |  \n",
      " |  copy : bool, default=True\n",
      " |      Set to False to perform inplace row normalization and avoid a\n",
      " |      copy (if the input is already a numpy array).\n",
      " |  \n",
      " |  clip : bool, default=False\n",
      " |      Set to True to clip transformed values of held-out data to\n",
      " |      provided `feature range`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  min_ : ndarray of shape (n_features,)\n",
      " |      Per feature adjustment for minimum. Equivalent to\n",
      " |      ``min - X.min(axis=0) * self.scale_``\n",
      " |  \n",
      " |  scale_ : ndarray of shape (n_features,)\n",
      " |      Per feature relative scaling of the data. Equivalent to\n",
      " |      ``(max - min) / (X.max(axis=0) - X.min(axis=0))``\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *scale_* attribute.\n",
      " |  \n",
      " |  data_min_ : ndarray of shape (n_features,)\n",
      " |      Per feature minimum seen in the data\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *data_min_*\n",
      " |  \n",
      " |  data_max_ : ndarray of shape (n_features,)\n",
      " |      Per feature maximum seen in the data\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *data_max_*\n",
      " |  \n",
      " |  data_range_ : ndarray of shape (n_features,)\n",
      " |      Per feature range ``(data_max_ - data_min_)`` seen in the data\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *data_range_*\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  n_samples_seen_ : int\n",
      " |      The number of samples processed by the estimator.\n",
      " |      It will be reset on new calls to fit, but increments across\n",
      " |      ``partial_fit`` calls.\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  minmax_scale : Equivalent function without the estimator API.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  NaNs are treated as missing values: disregarded in fit, and maintained in\n",
      " |  transform.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.preprocessing import MinMaxScaler\n",
      " |  >>> data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
      " |  >>> scaler = MinMaxScaler()\n",
      " |  >>> print(scaler.fit(data))\n",
      " |  MinMaxScaler()\n",
      " |  >>> print(scaler.data_max_)\n",
      " |  [ 1. 18.]\n",
      " |  >>> print(scaler.transform(data))\n",
      " |  [[0.   0.  ]\n",
      " |   [0.25 0.25]\n",
      " |   [0.5  0.5 ]\n",
      " |   [1.   1.  ]]\n",
      " |  >>> print(scaler.transform([[2, 2]]))\n",
      " |  [[1.5 0. ]]\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      MinMaxScaler\n",
      " |      sklearn.base.OneToOneFeatureMixin\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.utils._set_output._SetOutputMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.utils._metadata_requests._MetadataRequester\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, feature_range=(0, 1), *, copy=True, clip=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Compute the minimum and maximum to be used for later scaling.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          The data used to compute the per-feature minimum and maximum\n",
      " |          used for later scaling along the features axis.\n",
      " |      \n",
      " |      y : None\n",
      " |          Ignored.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted scaler.\n",
      " |  \n",
      " |  inverse_transform(self, X)\n",
      " |      Undo the scaling of X according to feature_range.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Input data that will be transformed. It cannot be sparse.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Xt : ndarray of shape (n_samples, n_features)\n",
      " |          Transformed data.\n",
      " |  \n",
      " |  partial_fit(self, X, y=None)\n",
      " |      Online computation of min and max on X for later scaling.\n",
      " |      \n",
      " |      All of X is processed as a single batch. This is intended for cases\n",
      " |      when :meth:`fit` is not feasible due to very large number of\n",
      " |      `n_samples` or because X is read from a continuous stream.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          The data used to compute the mean and standard deviation\n",
      " |          used for later scaling along the features axis.\n",
      " |      \n",
      " |      y : None\n",
      " |          Ignored.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted scaler.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Scale features of X according to feature_range.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Input data that will be transformed.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Xt : ndarray of shape (n_samples, n_features)\n",
      " |          Transformed data.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.OneToOneFeatureMixin:\n",
      " |  \n",
      " |  get_feature_names_out(self, input_features=None)\n",
      " |      Get output feature names for transformation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      input_features : array-like of str or None, default=None\n",
      " |          Input features.\n",
      " |      \n",
      " |          - If `input_features` is `None`, then `feature_names_in_` is\n",
      " |            used as feature names in. If `feature_names_in_` is not defined,\n",
      " |            then the following input feature names are generated:\n",
      " |            `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n",
      " |          - If `input_features` is an array-like, then `input_features` must\n",
      " |            match `feature_names_in_` if `feature_names_in_` is defined.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_names_out : ndarray of str objects\n",
      " |          Same as input features.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.OneToOneFeatureMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit to data, then transform it.\n",
      " |      \n",
      " |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      " |      and returns a transformed version of `X`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Input samples.\n",
      " |      \n",
      " |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      " |          Target values (None for unsupervised transformations).\n",
      " |      \n",
      " |      **fit_params : dict\n",
      " |          Additional fit parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      " |  \n",
      " |  set_output(self, *, transform=None)\n",
      " |      Set output container.\n",
      " |      \n",
      " |      See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n",
      " |      for an example on how to use the API.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      transform : {\"default\", \"pandas\"}, default=None\n",
      " |          Configure output of `transform` and `fit_transform`.\n",
      " |      \n",
      " |          - `\"default\"`: Default output format of a transformer\n",
      " |          - `\"pandas\"`: DataFrame output\n",
      " |          - `None`: Transform configuration is unchanged\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      " |  \n",
      " |  __init_subclass__(auto_wrap_output_keys=('transform',), **kwargs) from builtins.type\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |      Helper for pickle.\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  __sklearn_clone__(self)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |  \n",
      " |  get_metadata_routing(self)\n",
      " |      Get metadata routing of this object.\n",
      " |      \n",
      " |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      routing : MetadataRequest\n",
      " |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      " |          routing information.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "#Each scaler has some options, use help or refer to scikit-learn manual!\n",
    "help(preprocessing.MinMaxScaler) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non standardised\n",
      " [[ 0.  1. 13. 13. 10.  0.  0.  0.  0.  1. 13. 16. 15.  0.  0.  0.  0.  0.\n",
      "  12. 16. 16.  0.  0.  0.  0.  0. 16. 16. 12.  0.  0.  0.  0.  0. 15. 16.\n",
      "  13.  1.  0.  0.  0.  0. 15. 16. 11.  0.  0.  0.  0.  0. 16. 16. 16.  5.\n",
      "   0.  0.  0.  0. 14. 16. 15.  8.  1.  0.]\n",
      " [ 0.  0.  7. 15. 15.  2.  0.  0.  0.  0. 13.  6. 12.  6.  0.  0.  0.  0.\n",
      "   0.  0. 15.  2.  0.  0.  0.  0.  0. 13. 10.  0.  0.  0.  0.  0.  0.  8.\n",
      "  15. 12.  0.  0.  0.  3.  7.  0.  2. 15.  1.  0.  0.  2. 15.  6.  6. 16.\n",
      "   1.  0.  0.  0.  4. 15. 16.  7.  0.  0.]\n",
      " [ 0.  0. 11.  7.  0.  0.  0.  0.  0.  5. 16.  3.  2. 14.  3.  0.  0.  9.\n",
      "  15.  0. 12. 15.  0.  0.  0.  6. 16. 15. 16.  5.  0.  0.  0.  0.  6. 15.\n",
      "  11.  0.  0.  0.  0.  0.  1. 16.  4.  0.  0.  0.  0.  0.  9. 12.  0.  0.\n",
      "   0.  0.  0.  0. 13. 10.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1. 13. 12.  5.  0.  0.  0.  0. 11. 16.  4. 13.  2.  0.  0.  2.\n",
      "  16.  4.  0.  8.  5.  0.  0.  7. 12.  0.  0.  8.  8.  0.  0.  6. 12.  0.\n",
      "   0.  5.  8.  0.  0.  3. 16.  0.  0.  8.  7.  0.  0.  1. 15.  8.  6. 15.\n",
      "   3.  0.  0.  0.  2. 13. 15.  6.  0.  0.]\n",
      " [ 0.  0.  0. 14. 14.  1.  0.  0.  0.  0.  7. 16. 10.  2.  0.  0.  0.  0.\n",
      "  14. 14.  1.  0.  0.  0.  0.  0. 14. 16. 14.  4.  0.  0.  0.  1. 16. 16.\n",
      "   8. 16.  2.  0.  0.  0. 14. 11.  0. 13.  9.  0.  0.  0.  9. 14.  6. 16.\n",
      "   7.  0.  0.  0.  0. 14. 16. 14.  0.  0.]]\n",
      "Standardised\n",
      " [[0.         0.125      0.8125     0.8125     0.625      0.\n",
      "  0.         0.         0.         0.06666667 0.8125     1.\n",
      "  0.9375     0.         0.         0.         0.         0.\n",
      "  0.75       1.         1.         0.         0.         0.\n",
      "  0.         0.         1.         1.         0.75       0.\n",
      "  0.         0.         0.         0.         0.9375     1.\n",
      "  0.8125     0.0625     0.         0.         0.         0.\n",
      "  0.9375     1.         0.6875     0.         0.         0.\n",
      "  0.         0.         1.         1.         1.         0.3125\n",
      "  0.         0.         0.         0.         0.875      1.\n",
      "  0.9375     0.5        0.0625     0.        ]\n",
      " [0.         0.         0.4375     0.9375     0.9375     0.125\n",
      "  0.         0.         0.         0.         0.8125     0.375\n",
      "  0.75       0.375      0.         0.         0.         0.\n",
      "  0.         0.         0.9375     0.125      0.         0.\n",
      "  0.         0.         0.         0.8125     0.625      0.\n",
      "  0.         0.         0.         0.         0.         0.5\n",
      "  0.9375     0.75       0.         0.         0.         0.1875\n",
      "  0.4375     0.         0.125      0.9375     0.0625     0.\n",
      "  0.         0.125      0.9375     0.375      0.375      1.\n",
      "  0.0625     0.         0.         0.         0.25       0.9375\n",
      "  1.         0.4375     0.         0.        ]\n",
      " [0.         0.         0.6875     0.4375     0.         0.\n",
      "  0.         0.         0.         0.33333333 1.         0.1875\n",
      "  0.125      0.875      0.1875     0.         0.         0.5625\n",
      "  0.9375     0.         0.75       0.9375     0.         0.\n",
      "  0.         0.4        1.         0.9375     1.         0.3125\n",
      "  0.         0.         0.         0.         0.375      0.9375\n",
      "  0.6875     0.         0.         0.         0.         0.\n",
      "  0.0625     1.         0.25       0.         0.         0.\n",
      "  0.         0.         0.5625     0.75       0.         0.\n",
      "  0.         0.         0.         0.         0.8125     0.625\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.0625     0.8125     0.75       0.3125\n",
      "  0.         0.         0.         0.         0.6875     1.\n",
      "  0.25       0.8125     0.125      0.         0.         0.125\n",
      "  1.         0.25       0.         0.5        0.3125     0.\n",
      "  0.         0.46666667 0.75       0.         0.         0.5\n",
      "  0.53333333 0.         0.         0.42857143 0.75       0.\n",
      "  0.         0.3125     0.57142857 0.         0.         0.1875\n",
      "  1.         0.         0.         0.5        0.4375     0.\n",
      "  0.         0.0625     0.9375     0.5        0.375      0.9375\n",
      "  0.1875     0.         0.         0.         0.125      0.8125\n",
      "  0.9375     0.375      0.         0.        ]\n",
      " [0.         0.         0.         0.875      0.875      0.0625\n",
      "  0.         0.         0.         0.         0.4375     1.\n",
      "  0.625      0.125      0.         0.         0.         0.\n",
      "  0.875      0.875      0.0625     0.         0.         0.\n",
      "  0.         0.         0.875      1.         0.875      0.25\n",
      "  0.         0.         0.         0.07142857 1.         1.\n",
      "  0.5        1.         0.14285714 0.         0.         0.\n",
      "  0.875      0.6875     0.         0.8125     0.5625     0.\n",
      "  0.         0.         0.5625     0.875      0.375      1.\n",
      "  0.4375     0.         0.         0.         0.         0.875\n",
      "  1.         0.875      0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "scaler = preprocessing.MinMaxScaler().fit(digits_X_train2)\n",
    "digits_X_scaled = scaler.transform(digits_X_train2)\n",
    "digits_X_test_scaled = scaler.transform(digits_X_test2)\n",
    "\n",
    "print(\"Non standardised\\n\",digits_X_test2[:5,:])\n",
    "print(\"Standardised\\n\",digits_X_test_scaled[:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning\n",
    "Regression and classification in scikit-learn follow a [similar pattern](https://scikit-learn.org/stable/glossary.html#class-apis-and-estimator-types). Different methods are based on instances of the `Estimator` class, which provides methods to fit the estimator - `fit()`, to predict on new data - `predict()`, and to score the model - `score()`.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy: 0.9666203059805285\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "logistic = linear_model.LogisticRegression(C=1e5,max_iter=500)\n",
    "logistic.fit(digits_X_scaled, digits_y_train2)\n",
    "print(\"Classification accuracy:\",logistic.score(digits_X_test_scaled,digits_y_test2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      class0       1.00      0.99      0.99        71\n",
      "      class1       0.89      1.00      0.94        73\n",
      "      class2       1.00      0.99      0.99        71\n",
      "      class3       0.99      0.96      0.97        73\n",
      "      class4       0.99      0.96      0.97        72\n",
      "      class5       0.96      0.95      0.95        73\n",
      "      class6       0.99      0.97      0.98        72\n",
      "      class7       0.96      0.97      0.97        72\n",
      "      class8       0.97      0.93      0.95        70\n",
      "      class9       0.95      0.96      0.95        72\n",
      "\n",
      "    accuracy                           0.97       719\n",
      "   macro avg       0.97      0.97      0.97       719\n",
      "weighted avg       0.97      0.97      0.97       719\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "target_names = ['class0','class1','class2','class3','class4','class5','class6','class7','class8','class9']\n",
    "print(metrics.classification_report(digits_y_test2, logistic.predict(digits_X_test_scaled), target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
